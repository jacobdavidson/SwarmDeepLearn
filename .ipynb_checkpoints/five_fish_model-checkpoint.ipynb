{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Authors: Arthur Mateos and Chris Zhang\n",
    "#\n",
    "# Date: 27 July, 2016\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeplearn/.local/lib/python3.5/site-packages/pandas/core/computation/__init__.py:18: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paths to files with containing fish data\n",
    "datafiles = ['~/JolleFishData-5fish/CM1FRE_150324_1147_RP10_S04_G22_P.csv',\n",
    "            '~/JolleFishData-5fish/CM1FRE_150324_1227_RP09_S05_G05_P.csv',\n",
    "            '~/JolleFishData-5fish/CM1FRE_150324_1227_RP10_S05_G20_P.csv',\n",
    "            '~/JolleFishData-5fish/CM1FRE_150324_1307_RP09_S06_G12_P.csv']\n",
    "\n",
    "# Path to where the model is saved\n",
    "model_path = 'ModelCheckpoints/five_fish_model.ckpt'\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.4\n",
    "n_minibatches = 500\n",
    "minibatch_size = 512\n",
    "display_step = 10\n",
    "\n",
    "# Network parameters\n",
    "n_input = 10           # number of dimensions in input\n",
    "n_embedded = 64        # number of dimensions in which to embed input\n",
    "n_output = 10          # number of dimensions in output\n",
    "n_per_hidden = 256     # number of nodes per hidden layer\n",
    "    # Note: we require that all hidden layers have the same number of nodes\n",
    "n_hidden_layers = 2    # number of hidden layers\n",
    "l2_coefficient = 0.001 # coefficient for l2 loss normalization\n",
    "window_length = 50     # length of lookback window to give the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Input Data into the Right Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data(filename):\n",
    "    \"\"\"Download the csv file stored at 'filename'.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The location of the file to read.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the downloaded data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(filename, sep=\",\", header=0, index_col=0)\n",
    "    # Drop unneccesary columns\n",
    "    data = data.drop(['Color', 'col'], axis=1)\n",
    "    return data\n",
    "\n",
    "def flatten_data(data):\n",
    "    \"\"\"Flatten data from long to wide format.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Flattened DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    pivoted = data.pivot(columns=\"ID\")  # collapse on ID column\n",
    "    col_names = list(pivoted.columns.map('{0[0]}-{0[1]}'.format))  # extract column names from column index\n",
    "    pivoted.columns = col_names\n",
    "    return pivoted\n",
    "\n",
    "def add_delta_pos(data):\n",
    "    \"\"\"Add change-in-position columns for each fish.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with change-in-position columns added.\n",
    "    \"\"\"\n",
    "    \n",
    "    for col in data.columns:\n",
    "        data['d' + col] = data[col].diff()\n",
    "    data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_windows(windows):\n",
    "    \"\"\"Normalize the data in a given window, so that all points except possibly the last lie in [0,1]\"\"\"\n",
    "    normalized_windows = []\n",
    "    for window in windows:\n",
    "        mins = window[:-1,:].min(axis=0)   # leave out the last row when normalizing\n",
    "        maxes = window[:-1,:].max(axis=0)  # leave out the last row when normalizing\n",
    "        normalized_window = (window-mins)/(maxes-mins)\n",
    "        normalized_windows.append(normalized_window)\n",
    "    return normalized_windows\n",
    "\n",
    "def partition_windows(windows, window_length, train_percent, valid_percent, test_percent):\n",
    "    \"Partition data into train, validation, and test.\"\n",
    "    n_windows = len(windows)\n",
    "    possible_overlap = 2*window_length\n",
    "    n_windows -= possible_overlap\n",
    "    \n",
    "    n_train = n_windows*train_percent//100\n",
    "    n_valid = n_windows*valid_percent//100\n",
    "    n_test  = n_windows*test_percent//100\n",
    "\n",
    "    train = windows[:n_train,:,:]\n",
    "    valid = windows[n_train+window_length:n_train+n_valid+window_length,:,:]\n",
    "    test  = windows[n_train+n_valid+2*window_length:,:,:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "\n",
    "def download_and_preprocess_data(filenames=datafiles, window_length=window_length, normalize=False):\n",
    "    \"\"\"Download data from csv.\n",
    "    Add columns for delta position.\n",
    "    Break into windows, each of length 'window_length'.\n",
    "    Partition into training, validation, and test sets\"\"\"\n",
    "    \n",
    "    # List to hold windows, each containing window_length consecutive timesteps of data\n",
    "    windows = []\n",
    "    \n",
    "    # Iterate through all data files\n",
    "    for file in filenames:\n",
    "        data = download_data(file)\n",
    "        data = flatten_data(data)\n",
    "        data = add_delta_pos(data)\n",
    "\n",
    "        # Create one window for each sequence of length window_length\n",
    "        for index in range(len(data) - window_length):\n",
    "            windows.append(np.array(data.iloc[index:index+window_length,:]))\n",
    "    \n",
    "    if normalize:\n",
    "        windows = normalize_windows(windows)\n",
    "    \n",
    "    windows = np.array(windows)\n",
    "    \n",
    "    # 80% training, 10% test, 10% validation\n",
    "    train, valid, test = partition_windows(windows, window_length, 80, 10, 10)\n",
    "\n",
    "    # randomize the order\n",
    "    np.random.shuffle(train)\n",
    "    np.random.shuffle(valid)\n",
    "    np.random.shuffle(test)\n",
    "    \n",
    "    # Select data of interest:\n",
    "        # first 10 columns (x position, y position for each fish) as input\n",
    "        # last 10 columns (delta-x, delta-y for each fish) as output\n",
    "    # For each window, x contains all but last timestep, y contains only last timestep\n",
    "    x_train = train[:, :-1, :n_input]\n",
    "    y_train = train[:,  -1, -n_output:]\n",
    "    x_valid = valid[:, :-1, :n_input]\n",
    "    y_valid = valid[: , -1, -n_output:]\n",
    "    x_test  =  test[: ,:-1, :n_input]\n",
    "    y_test  =  test[: , -1, -n_output:]\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_fit(ground_truth, predicted):\n",
    "    \"\"\"Plot the accuracy of predictions, versus ground truth, for each of the predicted variables.\n",
    "    \n",
    "    TODO: add types here\n",
    "    Args:\n",
    "        ground truth (TYPE):\n",
    "        predicted (TYPE):\n",
    "        \n",
    "    Returns:\n",
    "        None.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert(len(ground_truth) == len(predicted))\n",
    "\n",
    "    n_points = len(ground_truth)\n",
    "    \n",
    "    # Plot each predicted variable separately\n",
    "    for series_index in range(ground_truth.shape[1]):\n",
    "        plt.title('Dataseries' + str(series_index) + \": predicted vs. ground truth\")\n",
    "            # TODO: make graph title more descriptive...\n",
    "        plt.xlabel('Ground truth value')\n",
    "        plt.ylabel('Predicted value')\n",
    "        \n",
    "        x_data = ground_truth[:,series_index].reshape(n_points)\n",
    "        y_data = predicted[:,series_index].reshape(n_points)\n",
    "        abline = [x for x in x_data]  # line of slope 1 and y-intercept 0\n",
    "        \n",
    "        # Plot predicted vs. ground truth\n",
    "        plt.scatter(x_data, y_data, color='black')\n",
    "        # Plot line of best fit\n",
    "        plt.plot(np.unique(x_data), np.poly1d(\n",
    "            np.polyfit(x_data, y_data, 1))(np.unique(x_data)), color='red')\n",
    "        # Plot line of perfect fit (y = x)\n",
    "        plt.plot(x_data, abline, color='blue')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(x_data, y_data, minibatch_size):\n",
    "    \"\"\"Generate a minibatch.\n",
    "    \n",
    "    Args:\n",
    "        x_data (ndarray): x data to draw samples from\n",
    "        y_data (ndarray): y data to draw samples from\n",
    "        minibatch_size (int): Size of the minibatch.\n",
    "        \n",
    "    Returns:\n",
    "        (ndarray, ndarray): input minibatch, output minibatch\n",
    "    \"\"\"\n",
    "    assert(len(x_data) == len(y_data))\n",
    "    assert(minibatch_size <= len(x_data))\n",
    "    \n",
    "    inputs = np.empty((minibatch_size, window_length-1, n_input))\n",
    "    outputs = np.empty((minibatch_size, n_output))\n",
    "    \n",
    "    # Select minibatch_size random windows from the training set\n",
    "    rand_indices = random.sample(range(len(x_data)), minibatch_size)\n",
    "    for index in range(minibatch_size):\n",
    "        inputs[index,:,:] = x_data[rand_indices[index],:,:]\n",
    "        outputs[index,:] = y_data[rand_indices[index],:]\n",
    "        \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run the Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data\n",
    "    x_batch_placeholder = tf.placeholder(tf.float32,\n",
    "                                      shape=(None, window_length-1, n_input))\n",
    "        # None so that able to hold differently sized batches\n",
    "    y_batch_placeholder = tf.placeholder(tf.float32, shape=(None, n_output))\n",
    "        # None so that able to hold differently sized batches\n",
    "    dropout_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables to be trained\n",
    "    embed_weights = tf.Variable(tf.truncated_normal([n_input, n_embedded]))\n",
    "    embed_biases = tf.Variable(tf.zeros([n_embedded]))\n",
    "    weights = tf.Variable(tf.truncated_normal([n_per_hidden, n_output]))\n",
    "    biases = tf.Variable(tf.zeros([n_output]))\n",
    "    \n",
    "    # Build graph\n",
    "    cells = []\n",
    "    for _ in range(n_hidden_layers):\n",
    "        cell = rnn.BasicLSTMCell(n_per_hidden)\n",
    "        cell = rnn.DropoutWrapper(cell, output_keep_prob=1.0 - dropout_placeholder)\n",
    "        cells.append(cell)\n",
    "    cell = rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    ### Define ops to run forward pass\n",
    "    \n",
    "    # Embedding the input laying and adding ReLU nonlinearity \n",
    "    stacks = [tf.nn.relu(tf.matmul(x_batch_placeholder[:,i,:], embed_weights)\\\n",
    "                       + embed_biases) for i in range(window_length-1)]\n",
    "\n",
    "    embedd = tf.stack(stacks, axis=1)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embedd, dtype=tf.float32)\n",
    "    logits = tf.matmul(outputs[:,-1,:], weights) + biases\n",
    "    \n",
    "    # Define cost and optimizer\n",
    "    l2_loss = tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases)+tf.nn.l2_loss(embed_weights)\n",
    "    cost = tf.sqrt(tf.reduce_mean(tf.squared_difference(logits, y_batch_placeholder))) +\\\n",
    "        l2_coefficient*l2_loss # cost function is rms\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    # Define op to initialize global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Define Saver op class to save and restore model\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(\n",
    "    n_minibatches=n_minibatches,\n",
    "    display_step=display_step,\n",
    "    learning_rate=learning_rate, # I would recommend decreasing the learning rate as you train\n",
    "    minibatch_size=minibatch_size,\n",
    "    graph=graph,\n",
    "    restore_from_save=True, # Change this to false when you train a new model\n",
    "    restore_from_latest=False,\n",
    "    restore_path=model_path,\n",
    "    save_when_finished=True,\n",
    "    save_path=model_path):\n",
    "    \n",
    "    # Launch the graph\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        if restore_from_save:\n",
    "            if restore_from_latest:\n",
    "                restore_path = tf.train.latest_checkpoint('./ModelCheckpoints/')\n",
    "    #           restore_path = \"./ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy.ckpt-270\"\n",
    "            try:\n",
    "                saver.restore(sess, restore_path)\n",
    "                print(\"Model successfully restored from %s.\\nResuming training.\" % restore_path)\n",
    "            except tf.errors.NotFoundError:\n",
    "                print(\"Save file not found.\\nInitializing graph from scratch instead.\")\n",
    "                sess.run(init)\n",
    "                print(\"Global variables initialized.\\nCommencing training.\")\n",
    "        else:\n",
    "            sess.run(init)\n",
    "            print(\"Global variables initialized.\\nCommencing training.\")\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        for minibatch_idx in range(n_minibatches):\n",
    "            _x_batch, _y_batch = get_minibatch(x_train, y_train, minibatch_size)\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            feed_dict = {x_batch_placeholder: _x_batch, y_batch_placeholder: _y_batch, dropout_placeholder: dropout}\n",
    "            _train_cost, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "\n",
    "            if minibatch_idx % display_step == 0:\n",
    "                _valid_cost = sess.run(\n",
    "                    cost, feed_dict={x_batch_placeholder: x_valid, y_batch_placeholder: y_valid, dropout_placeholder: dropout})\n",
    "                print(\"Minibatch \" + str(minibatch_idx) + \", Minibatch cost = \" + \\\n",
    "                      \"{:.6f}\".format(_train_cost))\n",
    "                print(\"Minibatch \" + str(minibatch_idx) + \", Validation set cost = \" + \\\n",
    "                      \"{:.6f}\".format(_valid_cost))\n",
    "                _save_path = saver.save(sess, save_path, global_step=minibatch_idx)\n",
    "                print(\"Model saved in file: %s\" % _save_path)\n",
    "\n",
    "        if save_when_finished:\n",
    "            # Save model weights to disk\n",
    "            _save_path = saver.save(sess, save_path)\n",
    "            print(\"Model saved in file: %s\" % _save_path)\n",
    "\n",
    "        # Plot fit on validation data\n",
    "        print(\"\\nCurrent validation performance:\")\n",
    "        plot_fit(y_valid, logits.eval(feed_dict={x_batch_placeholder: x_valid, dropout_placeholder: 0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_row(seed, prediction):  # TODO: adapt this to new data format\n",
    "    old_last_row = seed[-1,:]\n",
    "    return old_last_row + prediction\n",
    "    \n",
    "def generate_next_point(seed, sess):\n",
    "    feed_dict = {x_batch_placeholder: seed, dropout_placeholder: 0}\n",
    "    _logits = logits.eval(session=sess, feed_dict=feed_dict)\n",
    "    return _logits\n",
    "    \n",
    "def shift_seed(old_seed, new_row):\n",
    "    return np.vstack([old_seed[1:,:], new_row])\n",
    "    \n",
    "def generate_prediction(seed, prediction_length, restore_path=model_path, progress_counter=20):\n",
    "    \"\"\"Starting from an unnormalized seed sequence and generate a new sequence of positions\"\n",
    "    Params:\n",
    "        seed: ndarray, shape (1, window_length-1, n_input)\n",
    "        prediction_length: integer, number of desired timesteps to generate\n",
    "        restore_path: string, location from which to load saved graph state\n",
    "        progress_counter: integer, indicates number of intervals to print progress in generating sequence\n",
    "    Returns:\n",
    "        array of predicted locations, of shape shape (prediction_length, n_output)\n",
    "    \"\"\"\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # load the variables\n",
    "        try:\n",
    "            saver.restore(sess, restore_path)\n",
    "            print(\"Model successfully restored from %s.\\nDisplaying current fit.\" % restore_path)\n",
    "        except tf.errors.NotFoundError:  # TODO: add type of error\n",
    "            print(\"Save file not found.\\nExiting.\")\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        show_interval = np.maximum(1,prediction_length//progress_counter)\n",
    "        \n",
    "        for index in range(prediction_length):\n",
    "            if index % show_interval == 0:\n",
    "                print(\"Generated\", index, \"of\", prediction_length, \"data points.\")\n",
    "            batch_seed = seed.reshape(1, window_length-1, n_input)  # cast to batch format (batch of size 1)\n",
    "            prediction = generate_next_point(batch_seed, sess)\n",
    "            pred_coords = get_new_row(seed, prediction)\n",
    "            predictions.append(np.reshape(pred_coords, pred_coords.size))  # must reshape to remove extra dimension\n",
    "            seed = shift_seed(seed, pred_coords)\n",
    "        print(\"Done!\")\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training, validation, and test sets\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = download_and_preprocess_data(\n",
    "    filenames=datafiles, window_length=window_length, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train as much as you need to. Often a few thousands minibatches were used. \n",
    "# Decrease the learning rate as you go.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global variables initialized.\n",
      "Commencing training.\n",
      "Minibatch 0, Minibatch cost = 5.837825\n",
      "Minibatch 0, Validation set cost = 5.733052\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA2.ckpt-0\n",
      "Minibatch 20, Minibatch cost = 5.149290\n",
      "Minibatch 20, Validation set cost = 5.231023\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA2.ckpt-20\n"
     ]
    }
   ],
   "source": [
    "train_model(n_minibatches=100,display_step=20,learning_rate=0.0001, restore_from_save=False, restore_from_latest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-60\n",
      "Model successfully restored from ./ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-60.\n",
      "Resuming training.\n",
      "Epoch 0, Minibatch cost = 4.581092\n",
      "Epoch 0, Validation set cost = 4.662611\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-0\n",
      "Epoch 20, Minibatch cost = 4.374648\n",
      "Epoch 20, Validation set cost = 4.410938\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-20\n",
      "Epoch 40, Minibatch cost = 4.086440\n",
      "Epoch 40, Validation set cost = 4.168250\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-40\n",
      "Epoch 60, Minibatch cost = 3.828678\n",
      "Epoch 60, Validation set cost = 3.942400\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-60\n",
      "Epoch 80, Minibatch cost = 3.622396\n",
      "Epoch 80, Validation set cost = 3.777213\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-80\n",
      "Epoch 100, Minibatch cost = 3.468058\n",
      "Epoch 100, Validation set cost = 3.661015\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-100\n",
      "Epoch 120, Minibatch cost = 3.359439\n",
      "Epoch 120, Validation set cost = 3.552513\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-120\n",
      "Epoch 140, Minibatch cost = 3.285823\n",
      "Epoch 140, Validation set cost = 3.489198\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-140\n",
      "Epoch 160, Minibatch cost = 3.391715\n",
      "Epoch 160, Validation set cost = 3.443759\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-160\n"
     ]
    }
   ],
   "source": [
    "train_model(n_minibatches=1000,display_step=20,learning_rate=0.0001, restore_from_save=True, restore_from_latest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-200\n",
      "Model successfully restored from ./ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-200.\n",
      "Resuming training.\n",
      "Epoch 0, Minibatch cost = 3.085219\n",
      "Epoch 0, Validation set cost = 3.377854\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-0\n",
      "Epoch 20, Minibatch cost = 3.028424\n",
      "Epoch 20, Validation set cost = 3.361949\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-20\n",
      "Epoch 40, Minibatch cost = 3.076978\n",
      "Epoch 40, Validation set cost = 3.326528\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-40\n",
      "Epoch 60, Minibatch cost = 3.081317\n",
      "Epoch 60, Validation set cost = 3.309201\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-60\n",
      "Epoch 80, Minibatch cost = 3.020753\n",
      "Epoch 80, Validation set cost = 3.287211\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-80\n",
      "Epoch 100, Minibatch cost = 2.974349\n",
      "Epoch 100, Validation set cost = 3.295312\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-100\n",
      "Epoch 120, Minibatch cost = 3.008546\n",
      "Epoch 120, Validation set cost = 3.290763\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-120\n",
      "Epoch 140, Minibatch cost = 3.043448\n",
      "Epoch 140, Validation set cost = 3.303072\n",
      "Model saved in file: ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-140\n"
     ]
    }
   ],
   "source": [
    "train_model(n_minibatches=1000,display_step=20,learning_rate=0.0001, restore_from_save=True, restore_from_latest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-80\n",
      "Model successfully restored from ./ModelCheckpoints/five_fish_xy-embed_64-relu_2x256-l2_norm-pred_five_fish_delta_xy-MORE_DATA.ckpt-80.\n",
      "Displaying current fit.\n",
      "Generated 0 of 5000 data points.\n",
      "Generated 125 of 5000 data points.\n",
      "Generated 250 of 5000 data points.\n",
      "Generated 375 of 5000 data points.\n",
      "Generated 500 of 5000 data points.\n",
      "Generated 625 of 5000 data points.\n",
      "Generated 750 of 5000 data points.\n",
      "Generated 875 of 5000 data points.\n",
      "Generated 1000 of 5000 data points.\n",
      "Generated 1125 of 5000 data points.\n",
      "Generated 1250 of 5000 data points.\n",
      "Generated 1375 of 5000 data points.\n",
      "Generated 1500 of 5000 data points.\n",
      "Generated 1625 of 5000 data points.\n",
      "Generated 1750 of 5000 data points.\n",
      "Generated 1875 of 5000 data points.\n",
      "Generated 2000 of 5000 data points.\n",
      "Generated 2125 of 5000 data points.\n",
      "Generated 2250 of 5000 data points.\n",
      "Generated 2375 of 5000 data points.\n",
      "Generated 2500 of 5000 data points.\n",
      "Generated 2625 of 5000 data points.\n",
      "Generated 2750 of 5000 data points.\n",
      "Generated 2875 of 5000 data points.\n",
      "Generated 3000 of 5000 data points.\n",
      "Generated 3125 of 5000 data points.\n",
      "Generated 3250 of 5000 data points.\n",
      "Generated 3375 of 5000 data points.\n",
      "Generated 3500 of 5000 data points.\n",
      "Generated 3625 of 5000 data points.\n",
      "Generated 3750 of 5000 data points.\n",
      "Generated 3875 of 5000 data points.\n",
      "Generated 4000 of 5000 data points.\n",
      "Generated 4125 of 5000 data points.\n",
      "Generated 4250 of 5000 data points.\n",
      "Generated 4375 of 5000 data points.\n",
      "Generated 4500 of 5000 data points.\n",
      "Generated 4625 of 5000 data points.\n",
      "Generated 4750 of 5000 data points.\n",
      "Generated 4875 of 5000 data points.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Generate a trajectory\n",
    "\n",
    "savefile = './ModelOutputs/generated_trajectory.csv'\n",
    "seed = x_valid[0]  # an initial window to feed into the generative model\n",
    "pred = generate_prediction(seed, prediction_length=5000, restore_path=model_path, progress_counter=40)\n",
    "np.savetxt(savefile, pred, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Authors: Arthur Mateos and Chris Zhang\n",
    "#\n",
    "# Date: 27 July, 2016\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: This file is largely the same as five_fish_model.  See that file for more detailed comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readfile = '~/python/tensorflow/JolleFishData/170202_pi11_SOLI_S17_F175_TR.csv'\n",
    "model_path = 'ModelCheckpoints/four_border_distance_pred_delta_xy-64-all.ckpt'\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "dropout = 0.2\n",
    "n_minibatches = 100\n",
    "minibatch_size = 512\n",
    "display_step = 10\n",
    "l2_parameter = 0.001\n",
    "\n",
    "# Network parameters\n",
    "n_input = 4            # number of dimensions in input\n",
    "n_output = 2           # number of dimensions in output\n",
    "n_embedded = 256\n",
    "n_per_hidden = 64     # number of nodes per hidden layer\n",
    "    # Note: we require that all hidden layers have the same number of nodes\n",
    "n_hidden_layers = 2    # number of hidden layers\n",
    "window_length = 50    # length of lookback window to give the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Input Data into the Right Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data(filename):\n",
    "    \"\"\"Download the csv file stored at 'filename'.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The location of the file to read.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the downloaded data.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(readfile, sep=\",\", header=0, index_col=4)  # 'frame' is the 5th column in the csv\n",
    "    data = data.drop(['date', 'test', 'session', 'tank'], axis=1)\n",
    "        # these columns are constant over the whole file\n",
    "    return data\n",
    "    \n",
    "def add_wall_distances(data):\n",
    "    \"\"\"Add columns to dataframe 'data' that indicate fish's\n",
    "    distance from each of the four walls.\n",
    "    Includes sanity check to be verified by the user,\n",
    "    to ensure that inferred extremum values seem reasonable.\n",
    "    \"\"\"\n",
    "    \n",
    "    min_possible_x = (data['x'] - data['borderdistance']).min()\n",
    "    min_observed_x = data['x'].min()\n",
    "    max_possible_x = (data['x'] + data['borderdistance']).max()\n",
    "    max_observed_x = data['x'].max()\n",
    "    min_possible_y = (data['y'] - data['borderdistance']).min()\n",
    "    min_observed_y = data['y'].min()\n",
    "    max_possible_y = (data['y'] + data['borderdistance']).max()\n",
    "    max_observed_y = data['y'].max()\n",
    "\n",
    "    # TODO: pretty-ify this to make it more user-friendly\n",
    "    print('Sanity check: do these values look reasonable to you?')\n",
    "    print('Inferred min x: ' + str(min_possible_x) + '; observed min x: ' + str(min_observed_x))\n",
    "    print('Inferred max x: ' + str(max_possible_x) + '; observed max x: ' + str(max_observed_x))\n",
    "    print('Inferred min y: ' + str(min_possible_y) + '; observed min y: ' + str(min_observed_y))\n",
    "    print('Inferred max y: ' + str(max_possible_y) + '; observed max y: ' + str(max_observed_y))\n",
    "    \n",
    "    data['xmin_borderdistance'] = data['x'] - min_possible_x\n",
    "    data['xmax_borderdistance'] = max_possible_x - data['x']\n",
    "    data['ymin_borderdistance'] = data['y'] - min_possible_y\n",
    "    data['ymax_borderdistance'] = max_possible_y - data['y']\n",
    "    \n",
    "    return data\n",
    "\n",
    "def add_delta_position(data):\n",
    "    \"\"\"Add change-in-position columns for each fish.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with change-in-position columns added.\n",
    "    \"\"\"\n",
    "    \n",
    "    data['delta_x'] = data['x'].diff()\n",
    "    data['delta_y'] = data['y'].diff()\n",
    "    \n",
    "    data = data.dropna()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_windows(windows):\n",
    "    \"\"\"normalize the data in a given window, so that all points except possibly the last lie in [0,1]\"\"\"\n",
    "    normalized_windows = []\n",
    "    for window in windows:\n",
    "        mins = window[:-1,:].min(axis=0)   # leave out the last row when normalizing\n",
    "        maxes = window[:-1,:].max(axis=0)  # leave out the last row when normalizing\n",
    "        normalized_window = (window-mins)/(maxes-mins)\n",
    "        normalized_windows.append(normalized_window)\n",
    "    return normalized_windows\n",
    "\n",
    "def partition_windows(windows, window_length, train_percent, valid_percent, test_percent):\n",
    "    \"Partition data into train, validation, and test.\"\n",
    "    n_windows = len(windows)\n",
    "    possible_overlap = 2*window_length\n",
    "    n_windows -= possible_overlap\n",
    "    \n",
    "    n_train = n_windows*train_percent//100\n",
    "    n_valid = n_windows*valid_percent//100\n",
    "    n_test  = n_windows*test_percent//100\n",
    "\n",
    "    train = windows[:n_train,:,:]\n",
    "    valid = windows[n_train+window_length:n_train+n_valid+window_length,:,:]\n",
    "    test  = windows[n_train+n_valid+2*window_length:,:,:]\n",
    "    \n",
    "    return train, valid, test\n",
    "    \n",
    "# TODO: find way to preserve column headers within windows\n",
    "def preprocess_data(data, window_length=window_length, normalize=False):\n",
    "    \"\"\"Download data from csv.\n",
    "    Add columns for delta position.\n",
    "    Break into windows, each of length 'window_length'.\n",
    "    Partition into training, validation, and test sets\"\"\"\n",
    "\n",
    "    windows = []\n",
    "    for index in range(len(data) - window_length):\n",
    "        windows.append(np.array(data.iloc[index:index+window_length,:]))\n",
    "    \n",
    "    if normalize:\n",
    "        windows = normalize_windows(windows)\n",
    "    \n",
    "    windows = np.array(windows)\n",
    "    \n",
    "    # 80% training, 10% test, 10% validation\n",
    "    train, valid, test = partition_windows(windows, window_length, 80, 10, 10)\n",
    "\n",
    "    # randomize the order\n",
    "    np.random.shuffle(train)\n",
    "    np.random.shuffle(valid)\n",
    "    np.random.shuffle(test)\n",
    "    \n",
    "    # select data of interest:\n",
    "        # 'xmin_borderdistance', 'xmax_borderdistance', 'ymin_borderdistance', 'ymax_borderdistance'\n",
    "        # for input, and\n",
    "        # 'x', 'y' for output\n",
    "    # x is everything except last row, y is the last row\n",
    "    x_train = train[:, :-1, 8:12]\n",
    "    y_train = train[:,  -1,12:]\n",
    "    x_valid = valid[:, :-1, 8:12]\n",
    "    y_valid = valid[: , -1,12:]\n",
    "    x_test  =  test[: ,:-1, 8:12]\n",
    "    y_test  =  test[: , -1,12:]\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_fit(true_data, predicted_data):\n",
    "    slope = 1\n",
    "    intercept = 0\n",
    "\n",
    "    n_points = len(true_data)\n",
    "    for series_index in range(true_data.shape[1]):\n",
    "        x_data = true_data[:,series_index].reshape(n_points)\n",
    "        y_data = predicted_data[:,series_index].reshape(n_points)\n",
    "        abline = [slope * x + intercept for x in x_data]  # line of slope 1 and y-intercept 0\n",
    "\n",
    "        print(\"\\nDisplaying graph for dataseries \" + str(series_index) + \":\")\n",
    "        plt.scatter(x_data, y_data, color='black')\n",
    "        plt.plot(np.unique(x_data), np.poly1d(\n",
    "            np.polyfit(x_data, y_data, 1))(np.unique(x_data)), color='red')  # line of best fit\n",
    "        plt.plot(x_data, abline, color='blue')  \n",
    "\n",
    "        plt.xlabel('Actual value')\n",
    "        plt.ylabel('Predicted value')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a random window\n",
    "def get_next_window(x_data, y_data):\n",
    "    index = np.random.randint(0, len(x_data))\n",
    "    inputs = x_data[index,:,:]\n",
    "    output = y_data[index,:] \n",
    "    return inputs, output\n",
    "    \n",
    "# Generate a batch of batch_size random windows\n",
    "def get_new_batch(x_data, y_data, minibatch_size):\n",
    "    inputs = np.empty((minibatch_size, window_length-1, n_input))\n",
    "    outputs = np.empty((minibatch_size, n_output))\n",
    "    for index in range(minibatch_size):\n",
    "        inputs[index,:,:], outputs[index,:] = get_next_window(x_data, y_data)\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 49, 128)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data\n",
    "    x_batch_placeholder = tf.placeholder(tf.float32,\n",
    "                                      shape=(None, window_length-1, n_input))\n",
    "        # None so that able to hold differently sized batches\n",
    "    y_batch_placeholder = tf.placeholder(tf.float32, shape=(None, n_output))\n",
    "        # None so that able to hold differently sized batches\n",
    "    dropout_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables to be trained\n",
    "    weights = tf.Variable(tf.truncated_normal([n_per_hidden, n_output]))\n",
    "    biases = tf.Variable(tf.zeros([n_output]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([n_input, n_embedded]))\n",
    "    \n",
    "    # Build graph\n",
    "    cells = []\n",
    "    for _ in range(n_hidden_layers):\n",
    "        cell = rnn.BasicLSTMCell(n_per_hidden)\n",
    "        cell = rnn.DropoutWrapper(cell, output_keep_prob=1.0 - dropout_placeholder)\n",
    "        cells.append(cell)\n",
    "    cell = rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    \n",
    "    # Define ops to run forward pass\n",
    "    stacks = []\n",
    "    for i in range(window_length-1):\n",
    "        stacks.append(tf.nn.relu(tf.matmul(x_batch_placeholder[:,i,:], weights2)))\n",
    "    embedd = tf.stack(stacks, axis=1)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embedd, dtype=tf.float32)\n",
    "    logits = tf.matmul(outputs[:,-1,:], weights) + biases\n",
    "    \n",
    "    # Define cost and optimizer\n",
    "    cost = tf.sqrt(tf.reduce_mean(tf.squared_difference(logits, y_batch_placeholder)))+l2_parameter*(tf.nn.l2_loss(weights)+tf.nn.l2_loss(biases)+tf.nn.l2_loss(weights2))  # cost function is rms\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    # Define op to initialize global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Define Saver op class to save and restore model\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    n_minibatches=n_minibatches,\n",
    "    display_step=display_step,\n",
    "    learning_rate=learning_rate,\n",
    "    minibatch_size=minibatch_size,\n",
    "    graph=graph,\n",
    "    restore_from_save=True,\n",
    "    restore_path=model_path,\n",
    "    save_when_finished=True,\n",
    "    save_path=model_path):\n",
    "    \n",
    "    # Launch the graph\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        if restore_from_save:\n",
    "            try:\n",
    "                saver.restore(sess, restore_path)\n",
    "                print(\"Model successfully restored from %s.\\nResuming training.\" % restore_path)\n",
    "            except tf.errors.NotFoundError:\n",
    "                print(\"Save file not found.\\nInitializing graph from scratch instead.\")\n",
    "                sess.run(init)\n",
    "                print(\"Global variables initialized.\\nCommencing training.\")\n",
    "        else:\n",
    "            sess.run(init)\n",
    "            print(\"Global variables initialized.\\nCommencing training.\")\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        for epoch_idx in range(n_minibatches):\n",
    "            _x_batch, _y_batch = get_new_batch(x_train, y_train, minibatch_size)\n",
    "\n",
    "            # Run optimization op (backprop)\n",
    "            feed_dict = {x_batch_placeholder: _x_batch, y_batch_placeholder: _y_batch, dropout_placeholder: dropout}\n",
    "            _train_cost, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "\n",
    "            if epoch_idx % display_step == 0:\n",
    "                _valid_cost = sess.run(\n",
    "                    cost, feed_dict={x_batch_placeholder: x_valid, y_batch_placeholder: y_valid, dropout_placeholder: dropout})\n",
    "                print(\"Epoch \" + str(epoch_idx) + \", Minibatch cost = \" + \\\n",
    "                      \"{:.6f}\".format(_train_cost))\n",
    "                print(\"Epoch \" + str(epoch_idx) + \", Validation set cost = \" + \\\n",
    "                      \"{:.6f}\".format(_valid_cost))\n",
    "\n",
    "        if save_when_finished:\n",
    "            # Save model weights to disk\n",
    "            _save_path = saver.save(sess, save_path)\n",
    "            print(\"Model saved in file: %s\" % _save_path)\n",
    "\n",
    "        # Plot fit on validation data\n",
    "        print(\"\\nCurrent validation performance:\")\n",
    "        plot_fit(y_valid, logits.eval(feed_dict={x_batch_placeholder: x_valid, dropout_placeholder: 0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_wall_distances(prediction, seed):\n",
    "    new_wall_distances = np.zeros(4)\n",
    "    new_wall_distances[0] = seed[-1,0]+prediction[0,0]  # new distance from min x\n",
    "    new_wall_distances[1] = seed[-1,1]-prediction[0,0]  # new distance from max x\n",
    "    new_wall_distances[2] = seed[-1,2]+prediction[0,1]  # new distance from min y\n",
    "    new_wall_distances[3] = seed[-1,3]-prediction[0,1]  # new distance from max y\n",
    "    \n",
    "    new_coords = np.array([new_wall_distances[0], new_wall_distances[2]])\n",
    "    \n",
    "    return new_wall_distances, new_coords\n",
    "    \n",
    "def generate_next_point(seed, sess):\n",
    "    random_scalar = 1/4\n",
    "    feed_dict = {x_batch_placeholder: seed, dropout_placeholder: 0}\n",
    "    _logits = logits.eval(session=sess, feed_dict=feed_dict)\n",
    "    _logits = _logits + np.random.randn(1,2)*random_scalar\n",
    "    # print (_logits.shape,type(_logits))\n",
    "    return _logits\n",
    "    \n",
    "def shift_seed(old_seed, new_row):\n",
    "    return np.vstack([old_seed[1:,:], new_row])\n",
    "    \n",
    "def generate_prediction(seed, prediction_length, restore_path=model_path, progress_counter=20):\n",
    "    \"\"\"Starting from an unnormalized seed sequence and generate a new sequence of positions\"\n",
    "    Params:\n",
    "        seed: ndarray, shape (1, window_length-1, n_input)\n",
    "        prediction_length: integer, number of desired timesteps to generate\n",
    "        restore_path: string, location from which to load saved graph state\n",
    "        progress_counter: integer, indicates number of intervals to print progress in generating sequence\n",
    "    Returns:\n",
    "        array of predicted locations, of shape shape (prediction_length, n_output)\n",
    "    \"\"\"\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # load the variables\n",
    "        try:\n",
    "            saver.restore(sess, restore_path)\n",
    "            print(\"Model successfully restored from %s.\\nGenerating sequence.\" % restore_path)\n",
    "        except tf.errors.NotFoundError:  # TODO: add type of error\n",
    "            print(\"Save file not found.\\nExiting.\")\n",
    "            return\n",
    "        \n",
    "        predictions = []\n",
    "        show_interval = np.maximum(1,prediction_length//progress_counter)\n",
    "        \n",
    "        for index in range(prediction_length):\n",
    "            if index % show_interval == 0:\n",
    "                print(\"Generated\", index, \"of\", prediction_length, \"data points.\")\n",
    "            batch_seed = seed.reshape(1, window_length-1, n_input)  # cast to batch format (batch of size 1)\n",
    "            prediction = generate_next_point(batch_seed, sess)\n",
    "            wall_distances, pred_coords = extract_wall_distances(prediction, seed)\n",
    "            predictions.append(pred_coords)\n",
    "            seed = shift_seed(seed, wall_distances)\n",
    "        print(\"Done!\")\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: do these values look reasonable to you?\n",
      "Inferred min x: 0.0; observed min x: 10.2\n",
      "Inferred max x: 775.0; observed max x: 772.4\n",
      "Inferred min y: 0.0; observed min y: 3.9\n",
      "Inferred max y: 515.3; observed max y: 507.4\n"
     ]
    }
   ],
   "source": [
    "# Load data, add required columns\n",
    "all_data = download_data(readfile)\n",
    "all_data = add_wall_distances(all_data)\n",
    "all_data = add_delta_position(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training, validation, and test sets\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess_data(\n",
    "    all_data, window_length=window_length, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(n_minibatches=100,display_step=20,learning_rate=0.0001, restore_from_save=False, restore_from_latest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Generate a trajectory\n",
    "\n",
    "savefile = './ModelOutputs/generated_trajectory.csv'\n",
    "seed = x_valid[0]  # an initial window to feed into the generative model\n",
    "pred = generate_prediction(seed, prediction_length=5000, restore_path=model_path, progress_counter=40)\n",
    "np.savetxt(savefile, pred, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
